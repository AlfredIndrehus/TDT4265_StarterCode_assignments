{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "![](./plots/Task1a_1.JPG)\n",
    "![](./plots/Task1a_2.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "The max pooling layer \n",
    "\n",
    "## task 1c)\n",
    "\n",
    "\n",
    "**Formulas for output of convolutional layer (taken from appendix) :**\n",
    "\n",
    "- $W_2 = [(W_1 - F_W + 2P_W)/S_W]+ 1$\n",
    "- $H_2 = [(H_1 - F_H + 2P_H)/S_H] + 1$\n",
    "\n",
    "\n",
    "\n",
    "*Solving these for PW and PH respectively, with $H_1 = H_2, W_1 = W_2, S_H = S_W = 1$*\n",
    "\n",
    "$P_W = (F_W-1)/2$\n",
    "\n",
    "$P_H = (F_H-1)/2$\n",
    "\n",
    "With $F_H = F_W = 7$\n",
    "\n",
    "$P_W = (7-1)/2 = 3 = P_H$\n",
    "\n",
    "**Thus, three layers of padding are needed**\n",
    "\n",
    "\n",
    "\n",
    "## task 1d)\n",
    "Spatial dimensions feature map layer 1 are given as 508 x 508.\n",
    "With $S_W = S_H = 1$ and $P_W = P_H = 0$,\n",
    "the formulas are then reduced to:\n",
    "- $W_2 = [(W_1 − F_W )/1] + 1$\n",
    "- $H_2 = [(H_1 − F_H )/1] + 1$\n",
    "\n",
    "With $W_1 = H_1 = 512$, and $W_2 = H_2 = 508$ and solving for $F_W$ and $F_H$ we get:\n",
    "\n",
    " - $F_W = 512 - 508 + 1$ = 5\n",
    " - $F_H = 512 - 508 + 1$ = 5 \n",
    "\n",
    " The spatial dimensions for the kernel are **5x5**.\n",
    "\n",
    "\n",
    "## task 1e)\n",
    "Using $W_1 = H_1 = 508$ as input to the subsampling layer, with $F_W = F_H = 2$ and $S_W = S_H = 2$, we can solve for $W_2$ and $H_1$ and get:\n",
    "\n",
    " - $W_2 = [(502 - 2 + 0) / 2 ] + 1 = 254$\n",
    " - $H_2 = [(502 - 2 + 0) / 2 ] + 1 = 254$\n",
    "\n",
    "Thus, the spatial dimensions are **254x254**.\n",
    "\n",
    "\n",
    "## task 1f)\n",
    "\n",
    "Using the output from the previous layer as input, namely $W_1 = H1 = 254$ and kernel sizes $F_H = F_W = 3$, $S_W = S_H = 1$ and $P_W = P_H = 0$ we can use the same formulas as before and get:\n",
    "\n",
    " - $W_2 = [(254 - 3 + 0)/1]+1 = 252$\n",
    " - $H_2 = [(254 - 3 + 0)/1]+1 = 252$\n",
    "\n",
    "The spatial dimensions of the feature maps in the second layer are 252 x 252. \n",
    "## task 1g)\n",
    "\n",
    "The number of parameters is the number of weights + biases. Assuming the network takes an RGB image ($C_1 = 3$) with a width of 32.\n",
    "\n",
    "We have a network with the following convolutional layers:\n",
    "- Conv2D: **32 filters**, 5x5 kernel size, padding = 2, stride = 1\n",
    " - Conv2D: **64 filters**, 5x5 kernel size, padding = 2, stride = 1 \n",
    " - Conv2D layer: **128 filters**, 5x5 kernel size, padding 2, stride 1\n",
    " - \n",
    "We also have two fully connected layers:\n",
    "- Fully connected: **64 hidden units**\n",
    "- Fully connected: **10 hidden units** \n",
    "\n",
    "For the first convolutional layer, each filter has $F_H$ x $F_W$ x $C_1$ = $5$ x $5$ x $3$ = $75$ weights, multiplied by the number of filters we get $75$x$32$ =$2400$ weights.\n",
    "\n",
    "The number of biases for each convolutional layer is the same as the number of output filters. The total number of parameters for the first layer is then $2400$ + $32$ = $2432$ parameters.\n",
    "\n",
    "Our second convolutional layer has $F_H$ x $F_W$ x $C_1$ = $5$ x $5$ x $32$ = $800$ weights, multiplied by each filter and adding the biases we get $800$ x $64$ + $64$ = $51264$ parameters.\n",
    "\n",
    "The last convolutional layer has \n",
    "$F_H$ x $F_W$ x $C_1$ = $5$ x $5$ x $64$ = $1600$ weights, multiplied by each filter and adding the biases we get $1600$ x $128$ + $128$ = $204928$ parameters.\n",
    "\n",
    "The parameters in the fullt connected layers are respectively\n",
    "\n",
    " - First layer: $128$ x $4$ $4$ x $64$  + $64$ = 131136 parameters\n",
    " - Second layer: $64$ x $10$ + $10$ = 650 parameters\n",
    "\n",
    "Thus, the total number of parameters is $2432$ + $51264$ + $204928$ + $131136$ + $650$ = **390410** parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2a)\n",
    "Plot showing validation loss and training loss. On the right the accuracy is also included:\n",
    "![](./plots/task2_plot_save.png)\n",
    "\n",
    "### Task 2b)\n",
    "\n",
    "Training Loss: 0.3922, Training Accuracy: 0.8699  \n",
    "Validation Loss: 0.7848, Validation Accuracy: 0.7354  \n",
    "Test Loss: 0.8025, Test Accuracy: 0.7327  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "| Layer | Layer Type   | Number of Hidden Units / Number of Filters | Activation Function |\n",
    "|-------|--------------|------------------------------------------|----------------------|\n",
    "| 1     | Conv2D       | 64                                       | ELU                  |\n",
    "|       | BatchNorm2d  |                                          |                      |\n",
    "| 2     | Conv2D       | 64                                       | ELU                  |\n",
    "|       | BatchNorm2d  |                                          |                      |\n",
    "|       | MaxPool2d    |                                          |                      |\n",
    "| 3     | Conv2D       | 128                                      | ELU                  |\n",
    "|       | BatchNorm2d  |                                          |                      |\n",
    "| 4     | Conv2D       | 128                                      | ELU                  |\n",
    "|       | BatchNorm2d  |                                          |                      |\n",
    "|       | MaxPool2D    |                                          |                      |\n",
    "| 5     | Conv2D       | 256                                      | ELU                  |\n",
    "|       | BatchNorm2d  |                                          |                      |\n",
    "| 6     | Conv2D       | 256                                      | ELU                  |\n",
    "|       | BatchNorm2d  |                                          |                      |\n",
    "|       | MaxPool2D    |                                          |                      |\n",
    "| 7     | Conv2D       | 512                                      | ELU                  |\n",
    "|       | BatchNorm2d  |                                          |                      |\n",
    "|       | Dropout      |                                          |                      |\n",
    "| 8     | Conv2D       | 512                                      | ELU                  |\n",
    "|       | BatchNorm2d  |                                          |                      |\n",
    "|       | MaxPool2D    |                                          |                      |\n",
    "| -     | Flatten      |                                          |                      |\n",
    "| 9    | Fully-Connected | 128                                    | ELU                  |\n",
    "| 10    | Fully-Connected | 10                                     | SoftMax               |\n",
    "\n",
    "\n",
    "### Training Details\n",
    "\n",
    "- **Optimizer**: Stochastic Gradient Descent (SGD) optimizer with weight decay (L2 regularization) of 1e-5\n",
    "- **Learning Rate**: 5e-2\n",
    "- **Batch Size**: 64\n",
    "- **Epochs**: 10\n",
    "- **Early Stopping**: Stopped after 4 consecutive epochs of no improvement in validation loss\n",
    "- **Dropout parameter (p)**: We used p=0,4 in the dropout in layer 7.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 3b)\n",
    "Final train loss, training accuracy, validation accuracy and test accuracy\n",
    "\n",
    "| Metric            | Training  | Validation | Test    |\n",
    "|-------------------|-----------|------------|---------|\n",
    "| Loss              | 0.4198    | 0.5530     | 0.5123  |\n",
    "| Accuracy          | 0.8505    | 0.8125     | 0.8279  |\n",
    "\n",
    "Plots for task3 model\n",
    "![](./plots/task3_model_plot.png)\n",
    "\n",
    "### Task 3c)\n",
    "##### L2 regularization\n",
    "We extend the optimizer in \"trainer.py\" like this \"self.optimizer = torch.optim.SGD(self.model.parameters(), self.learning_rate,weight_decay=1e-5)\". That is, we added weight_decay. This actually make the model performa slightly worse:  \n",
    "- Test Accuracy without L2: 0.8323  \n",
    "- Test Accuracy with L2: 0.8279  \n",
    "\n",
    "We find this result quite od, as we exptected L2 relularization to make the model performe better on test and validation data. L2 regularization encourages small weights, which is usefull for prevventing overfitting. Since it did not make our model performe better, we belive that the L2 regularization make the model overly simple.  \n",
    "\n",
    "##### Data Augmentation\n",
    "\n",
    "From data augmentation we added:\n",
    "- transforms.RandomHorizontalFlip()\n",
    "- transforms.RandomRotation(10)\n",
    "\n",
    "to the implementation of \"transform_train\" in the file \"dataloaders.py\". Here are the results with and without this extenction:\n",
    "\n",
    "\n",
    "|                           |       Test Loss  |  Test Accuracy    |\n",
    "|---------------------------|------------------|-------------------|\n",
    "|Without data augmentation  |   0.6089         |   0.7976          |\n",
    "|With data augmentatuion    |      0.4980      | 0.8323            |\n",
    "\n",
    "This shows that data augmentation has been very usefull in improving our model. When we apply this type of data augmentation, we alter the data by flipping and rotationg the images. This makes the model better at capturing the underlaying patterns regardles of rotation, which in turn gives better performence on unseen data.  \n",
    "\n",
    "##### Batch normalization\n",
    "This methode made our model performe better. We applyed \"BatchNorm2d\" after each convolutional layer. This is a batch normalization technique usefull for normalizing multidimensional spation input (such as RBG-images) accross several dimentions (channels).  \n",
    "This is usefull to prevent unintended covariance accros channels.  \n",
    "This technique proved to be very efficient in our model.  \n",
    "See task 3d for plot of performance with and without this.  \n",
    "\n",
    "##### Filter size\n",
    "We reduced the filtersize from 5 to 3. This mede the model performe slightly better.  \n",
    "\n",
    "##### Number of filters\n",
    "Increase the number of filters from 32 to 64.  This mede the model performe slightly better. Introduce more parameters, making the model more complex.\n",
    "\n",
    "##### Network architecture\n",
    "The architecture was changed as shown in task 3a). Made the model able to capture more complex patterns.\n",
    "\n",
    "##### Activation Functions\n",
    "We tried chainging the activation function from ReLU to ELU. This mede the model performe slightly better. Helped to address the \"dying ReLu problem\".\n",
    "\n",
    "### Task 3d)\n",
    "This task shows a plot of performance and loss before and after applying Batch normalization.  \n",
    "![](./plots/task3d.jpeg)  \n",
    "\n",
    "It shows significant improvements in both loss and accuracy.  \n",
    "\n",
    "### Task 3e)\n",
    "This is the same model as before, be reaced an acuracy of over 80% before reading task 3e, so we answered the previous task for this model.\n",
    "![](./plots/task3_model_plot.png)\n",
    "### Task 3f)\n",
    "\n",
    "From the plot above we observe that the validation loss is above the training loss, this it natural, but what we can also see is that the model gap between test and validation loss seems to increase towards the end of the graph. This can be a sign of the model starting to overfit towards the end of training. We belive that the model stoped training before this became a problem, as this is only happening towards the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "![](./plots/task4_plot.png)\n",
    "final test accuracy:  0.8901\n",
    "\n",
    "##### Parameters:\n",
    "- Omtomizer: Adam\n",
    "- Batch size: 32\n",
    "- Learning rate: $5*10^{-4}$\n",
    "- Data augemntation: RandomHorizontalFlip() and RandomRotation(10)\n",
    "- Number of epochs: 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
