{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Task1a.jpg)\n",
    "![](Task1a_2.jpg)\n",
    "![](Task1a_3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculations for task 2a)\n",
    "\n",
    "Mean:  33.5527\n",
    "Standard deviation: 78.8755\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "Number of parameters in network:\n",
    "Hidden layer (weights including bias-trick * number of neurons): 785 * 64\n",
    "Outputlayer (Weights * neurons/number of classes): 64 * 10\n",
    "785*64 + 64*10 = 50880"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3abc)\n",
    "\n",
    "The plots clearly show improvements with each addition of improvement. Just adding weight initialization alone resulted in quicker stopping time, reduced training loss and increased validation accuracy. Incrementally adding the other improvements also further improved the results, as expected. \n",
    "Using the improved sigmoid function seems to have the biggest impact on stopping time. This was also expected, as the improved sigmoid function should converge faster during training. Overall the model also seem even more stable with just improved weight initilization alone.  \n",
    "\n",
    "\n",
    "\n",
    "Note: With all 3 improvements added, the model seems to fluctuate quite a bit at first, both with validation accuracy as well as validation loss. We would expect the model to be more stable from the addition of momentum. We don't know if this is caused by a bug in the code, or something else. We suspect it mighte be a bug with the implementation of the momentum. The model did stop earlier with the addition of momentum, but we expected it to stop even earlier with this implmemented. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a) and 4b)\n",
    "![](Task4a_and4b.png)\n",
    "\n",
    "For the model of 32 units in the hidden layer, we can se that the training loss is greater across the training.\n",
    "Where as for the model with 128 units in hidden layer, the training loss goes towards zero fasterter. The accuracy also looks like it converges faster, and is higher for the model of 128 hidden units.\n",
    "\n",
    "If the number of hidden units is to low:\n",
    "We will tend to not capture the underlaying pattern in the data. THis will lead to high training loss, and low validation accuracy on unseen data\n",
    "\n",
    "If the number of hidden units is to high:\n",
    "The training loss will look realy good fast, as the model learns the pattern fast. But it will also overfit to the training data. This will in the end lead to a worse accuracy on unseen data (validation accuracy), as the model generalizes poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c, d and e)\n",
    "NOTE: we managed to update the forwar function to propperly handle several hidden layser. The Backward function however, we were not able to sucessfully extend to handle several hidden layers. We vere stuck on the same error for a long time, and decided to deliver the working implementation, the one that we have used for the rest of the task.\n",
    "\n",
    "We can however say what we think:\n",
    "\n",
    "d and e) We think that the network, with multiple hidden layers could potentially be improved a lot, because of several factors.\n",
    "The first is the addition of more parameters as a result of more layers. While this could also pose issues such as overfitting, especially as the number of layers increases. A more complex model could potentially introduce more issues. More complex networks could also potentially be more difficult to train, and would maybe require more resources (notably time) to finish. \n",
    "\n",
    "We would expect that a two-layer network with finely tuned hyperparameters is going to be easier to train, and would give us just as good results as a three layer network. We can also assume that as the number of layers increases, the training loss converges faster, but we assume the advantages would drop off as the number of layers increases. \n",
    "\n",
    "We also assume that increasing the number of layers could make the network prone to overfitting. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
